# -*- coding: utf-8 -*-
"""projekt1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ze0aLLJys_VwADFzvzZ7ozv3NDY5fQVi

WCZYTUJEMY POTRZEBNE BIBLIOTEKI
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
#from outliers import smirnov_grubbs as grubbs
from sklearn import linear_model
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.decomposition import PCA
from scipy.stats import levene
from scipy.stats import bartlett
from sklearn.model_selection import GridSearchCV
from sklearn.neighbors import KNeighborsRegressor
#from google.colab import drive
#drive.mount('/content/drive')

"""WCZYTUJEMY DANE Z TABELI"""

dane = pd.read_excel("data_encoded.xlsx", sheet_name="1000")
dane

n = len(dane.columns)

"""SPRAWDZAMY BRAKUJĄCE WARTOŚCI DLA KAŻDEJ TABELI"""

cols_missing = [col for col in dane.columns
                     if dane[col].isnull().any()]
for i in cols_missing:
    print(i,sum(dane[i].isnull()))

"""USUWAMY PIERWSZĄ KOLUMNĘ"""

dane.drop(columns=["Sample label"], axis=1, inplace=True)
dane

dane.rename(columns = {'EC50 vs 3T3 [mg/l]':'Endpoint1', 'EC50 vs HepG2 [mg/l]':'Endpoint2'}, inplace = True)
dane

"""STANDARYZACJA DANYCH"""

scaler = StandardScaler()
standardized_data1 = scaler.fit_transform(dane[dane.columns[0:5]])
standardized_data2 = scaler.fit_transform(dane[dane.columns[7:n]])
standardized_df1 = pd.DataFrame(standardized_data1, columns=dane[dane.columns[0:5]].columns)
standardized_df2 = pd.DataFrame(standardized_data2, columns=dane[dane.columns[7:n]].columns)

df = pd.concat([standardized_df1, standardized_df2, dane[dane.columns[5:7]]], axis=1)

"""TWORZENIE HEATMAPY KORELACJI"""

plt.subplots(figsize=(44,44))
heatmap=sns.heatmap(abs(df[df.columns[0:(n-3)]].corr()), annot=True)
heatmap.set_title('Correlation Heatmap', fontdict={'fontsize':40}, pad=12)

"""HOMOGENICZNOŚĆ DANYCH - TESTY LEVENE'A I BARTLETTA"""

df1=df.drop(['Endpoint1','Endpoint2'], axis=1)

alpha = 0.05

columns = df1.columns
data = [df1[col].values for col in columns]

statistic, p_value = levene(*data)

"""Wartość p poniżej 0.05 oznacza odrzucenie hipotezy zerowej"""

print("Statystyka testowa:", statistic)
print("Wartość p:", p_value)

statistic, p_value = bartlett(*data)

"""Jeśli wartość p w teście Bartletta jest mniejsza od poziomu istotności, to należy odrzucić hipotezę zerową i stwierdzić, że wariancje są różne. W przeciwnym razie, nie ma podstaw do odrzucenia hipotezy zerowej i można uznać, że wariancje są homogeniczne.




"""

print("Statystyka testowa:", statistic)
print("Wartość p:", p_value)

"""PCA"""

# Przygotowanie danych do podziału
X = df[df.columns[0:(n-3)]]
y = df[df.columns[42]]

# Pca = PCA(n_components=2)
pca = PCA()
pca.fit(X)
X_pca = pca.transform(X)

df_col1 = df[df.columns[42]]
col1 = df_col1.values.tolist()
plt.scatter(X_pca[:, 0], X_pca[:, 1],c=col1)
plt.xlabel('PCA 1')
plt.ylabel('PCA 2')
plt.colorbar(label='Endpoint1')
plt.show()

df_col2 = df[df.columns[43]]
col2 = df_col2.values.tolist()
plt.scatter(X_pca[:, 0], X_pca[:, 1],c=col2)
plt.xlabel('PCA 1')
plt.ylabel('PCA 2')
plt.colorbar(label='Endpoint2')
plt.show()

# Dla Endpoint1
pca.components_

max_waga = np.argmax(abs(pca.components_), axis=1)

najwazniejsza_cecha = df.columns[max_waga]
najwazniejsza_cecha

for i, ratio in enumerate(pca.explained_variance_ratio_):
    print(f"PC{i+1}: {ratio*100:.2f}%")

explained_variance = pca.explained_variance_ratio_
cumulative_variance = np.cumsum(explained_variance)
num_components = np.argmax(cumulative_variance >= 0.95) + 1
num_components

data1 = df[["Metals_SumIP", "Ag [%mol]", "number of dopants", "Pd [%mol]","Endpoint1"]].copy()

#X_pca[:, 0], X_pca[:, 1] - dwie główne składowe i na tym model zrobić (to nie są nazwyk kolumn, tylko kobiacja liniowa kilku kolumn)

# X_pca (ładne dane, chyba nawet standardowe)

# Skalowanie danych treningowych (jeszcze to trzeba zrobić dla y_train)
#standardized_datatren1 = scaler.fit_transform(dane_tren[dane_tren.columns[0:4]])
#standardized_dftren1 = pd.DataFrame(standardized_datatren1, columns=dane_tren[dane_tren.columns[0:4]].columns)

# Skalowanie danych testowych
#standardized_datatest1 = scaler.fit_transform(dane_test[dane_test.columns[0:4]]) # fit_transform -> transform
#standardized_dftest1 = pd.DataFrame(standardized_datatest1, columns=dane_test[dane_test.columns[0:4]].columns)

#knnc = KNeighborsRegressor(n_neighbors=7, weights='distance')
#knnc.fit(X_pca, y_train)

# Przewidywanie dla zbioru testowego i treningowego
#y_predknnc = knn.predict(X_pca)
#y_predknntc = knn.predict(X_train)

# Obliczenie metryk oceny jakości regresji
#mse = mean_squared_error(y_test, y_predknn)
#rmse = np.sqrt(mse)

#print("Root Mean Squared Error (RMSE):", rmse)

# Obliczanie R^2 - walidacja wewnętrzna
#r2 = r2_score(y_train, y_predknnt)
#print("R^2:", r2)

# Obliczanie Q^2 - walidacja zewnętrzna
#q2 = r2_score(y_test, y_predknn)
#print("Q^2:", q2)

# Obliczenie Q^2 CV - walidacja krzyżowa
#n = len(y_test)
#p = X_test.shape[1]
#Q2_CV = 1 - (1 - q2) * ((n - 1) / (n - p - 1))

#print("Q^2 CV:", Q2_CV)

"""PODZIAŁ NA DANE TESTOWE I TRENINGOWE + ODDZIELNA STANDARYZACJA"""

dane_test =data1[::5]    #data1 -> X_pca
dane_test.reset_index(drop=True, inplace=True)

dane_test

dane_tren = data1[~(data1.index % 5 == 0)]
dane_tren.reset_index(drop=True, inplace=True)

# Skalowanie danych treningowych
standardized_datatren1 = scaler.fit_transform(dane_tren[dane_tren.columns[0:4]])
standardized_dftren1 = pd.DataFrame(standardized_datatren1, columns=dane_tren[dane_tren.columns[0:4]].columns)

# Skalowanie danych testowych
standardized_datatest1 = scaler.fit_transform(dane_test[dane_test.columns[0:4]]) # fit_transform -> transform
standardized_dftest1 = pd.DataFrame(standardized_datatest1, columns=dane_test[dane_test.columns[0:4]].columns)

dftest = pd.concat([standardized_dftest1, dane_test[dane_test.columns[4]]], axis=1)
dftren = pd.concat([standardized_dftren1, dane_tren[dane_tren.columns[4]]], axis=1)

"""MODEL"""

X_train = pd.DataFrame(dftren[dftren.columns[0:4]], columns=dftren[dftren.columns[0:4]].columns)
ostatnia_kolumna = dftren.iloc[:, -1]

# Stwórz nową tabelę na podstawie ostatniej kolumny
y_train = pd.DataFrame(ostatnia_kolumna, columns=['Endpoint1'])

X_test = pd.DataFrame(dftest[dftest.columns[0:4]], columns=dftest[dftest.columns[0:4]].columns)
ostatnia_kolumna = dftest.iloc[:, -1]

# Stwórz nową tabelę na podstawie ostatniej kolumny
y_test = pd.DataFrame(ostatnia_kolumna, columns=['Endpoint1'])

"""......................."""

from sklearn import neighbors
from sklearn.metrics import r2_score
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error
from sklearn.svm import SVR
from sklearn.ensemble import RandomForestRegressor
import warnings
from sklearn.exceptions import DataConversionWarning
from sklearn.model_selection import LeaveOneOut
from sklearn.model_selection import cross_val_score

# Wyłączenie wyświetlania powiadomień DataConversionWarning
warnings.filterwarnings(action='ignore', category=DataConversionWarning)

y_train.shape

y_predknn.shape

knn = KNeighborsRegressor(n_neighbors=7, weights='distance')
knn.fit(X_train, y_train)

# Przewidywanie dla zbioru testowego i treningowego
y_predknn = knn.predict(X_test)
y_predknnt = knn.predict(X_train)

# Obliczenie metryk oceny jakości regresji
mse = mean_squared_error(y_test, y_predknn)
rmse = np.sqrt(mse)

print("Root Mean Squared Error (RMSE):", rmse)

# Obliczanie R^2 - walidacja wewnętrzna
r2 = r2_score(y_train, y_predknnt)
print("R^2:", r2)

# Obliczanie Q^2 - walidacja zewnętrzna
q2 = r2_score(y_test, y_predknn)
print("Q^2:", q2)

# Obliczenie Q^2 CV - walidacja krzyżowa
n = len(y_test)
p = X_test.shape[1]
Q2_CV = 1 - (1 - q2) * ((n - 1) / (n - p - 1))

print("Q^2 CV:", Q2_CV)

""".................................................."""

max_depth = 5
regressor = DecisionTreeRegressor(max_depth=max_depth)

# Trenowanie modelu na zbiorze treningowym
regressor.fit(X_train, y_train)

# Przewidywanie dla zbioru testowego i treningowego
y_preddt = regressor.predict(X_test)
y_preddtt = regressor.predict(X_train)

# Obliczenie metryk oceny jakości regresji
mse = mean_squared_error(y_test, y_preddt)
rmse = np.sqrt(mse)

print("Root Mean Squared Error (RMSE):", rmse)
# Obliczanie R^2
r2 = r2_score(y_train, y_preddtt)
print("R^2:", r2)

# Obliczanie Q^2
q2 = r2_score(y_test, y_preddt)
print("Q^2:", q2)

# Obliczenie Q^2 CV - walidacja krzyżowa

n = len(y_test)
p = X_test.shape[1]
Q2_CV = 1 - (1 - q2) * ((n - 1) / (n - p - 1))

print("Q^2 CV:", Q2_CV)

""".................................................."""

# Inicjalizacja modelu Lasu Losowego
rf = RandomForestRegressor(n_estimators=100, random_state=42)

# Trenowanie modelu na zbiorze treningowym
rf.fit(X_train, y_train)

# Przewidywanie dla zbioru testowego i treningowego
y_predrf = rf.predict(X_test)
y_predrft = rf.predict(X_train)

# Obliczenie średniego błędu kwadratowego
mse = mean_squared_error(y_test, y_predrf)
rmse = np.sqrt(mse)
print("Root Mean Squared Error:", rmse)

# Obliczanie R^2
r2 = r2_score(y_train, y_predrft)
print("R^2:", r2)

# Obliczanie Q^2
q2 = r2_score(y_test, y_predrf)
print("Q^2:", q2)

# Obliczenie Q^2 CV - walidacja krzyżowa
n = len(y_test)
p = X_test.shape[1]
Q2_CV = 1 - (1 - q2) * ((n - 1) / (n - p - 1))

print("Q^2 CV:", Q2_CV)

""".................................................."""

# Inicjalizacja modelu Support Vector Regression
#regressor = SVR(kernel='linear')

# Trenowanie modelu na zbiorze treningowym
#regressor.fit(X_train, y_train)

# Przewidywanie dla zbioru testowego
#y_predsvm = regressor.predict(X_test)

# Obliczenie metryk oceny jakości regresji
#mse = mean_squared_error(y_test, y_predsvm)
#rmse = np.sqrt(mse)

#print("Root Mean Squared Error (RMSE):", rmse)

# Obliczanie R^2
#q2 = r2_score(y_test, y_predsvm)
#print("Q^2:", q2)

y_preddttrans = y_preddt.reshape(-1, 1)
y_predrftrans = y_predrf.reshape(-1, 1)
#y_predsvmtrans = y_predsvm.reshape(-1, 1)

errors1 = y_test - y_predknn
errors2 = y_test - y_preddttrans
errors3 = y_test - y_predrftrans
#errors4 = y_test - y_predsvmtrans

# Wykres reszt
plt.scatter(y_test, errors1, color='blue', label='KNN')
plt.scatter(y_test, errors2, color='red', label='Drzewo decyzyjne')
plt.scatter(y_test, errors3, color='green', label='Las losowy')
#plt.scatter(y_test, errors4, color='orange', label='SVM')
plt.axhline(0, color='black', linestyle='--')
plt.xlabel('y_test')
plt.ylabel('Reszty')
plt.legend()
plt.title('Wykres reszt')
plt.show()

